{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddff9acd-fc17-4794-8eff-a07bb0a20117",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ab8a5-bf5c-4160-8588-2dc42de410a4",
   "metadata": {},
   "source": [
    "### The goal of this project is to build a sentiment analysis system for movie reviews using the IMDB dataset. The system should automatically classify each review as positive or negative, and investigate how different text representation techniques affect the performance of sentiment classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed356631-71d3-45c7-9860-a096fed7105c",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1370777-75db-4c7c-9eac-fbee5a980009",
   "metadata": {},
   "source": [
    "### 1.Data Preprocessing\n",
    "### 2.Text Representation\n",
    "### 3.Model Training\n",
    "### 4.Model Evalution\n",
    "### 5.Comparison and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21e22b-7a6f-4981-a702-53f7f50b69aa",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33908a-fb5e-4b48-b283-bb395b03b45c",
   "metadata": {},
   "source": [
    "### This project uses the IMDB Movie Reviews Dataset containing 50,000 movie reviews, each labeled as either positive or negative. The dataset has two main columns:\n",
    "### review – the text of the movie review\n",
    "### sentiment – the target label (\"positive\" or \"negative\"), which is converted to a numeric label (1 = positive, 0 = negative) for modeling.\n",
    "### For experiments, a smaller subset of the reviews is sampled and then split into training (80%) and testing (20%) while keeping the class balance approximately 50–50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0d71e-c872-4a50-a88b-3087c05c7d40",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eedc12-cf40-45bc-9302-39e01ca58a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import random\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ae922-0a05-4027-9ec2-0d04b2a13271",
   "metadata": {},
   "source": [
    "#  Load & preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828a4369-cca4-465a-a2e9-5666e34cff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 50000\n",
      "Using rows: 20000\n",
      "Train size: 16000\n",
      "Test size : 4000\n",
      "\n",
      "Example raw review:\n",
      " Im a huge M Lillard fan that's why I ended up watching this movie. Honestly I doubt that if he wasn't in the movie i would of enjoyed it as much or even watched it but once I did watch it realize the \n",
      "\n",
      "Example cleaned review:\n",
      " im a huge m lillard fan that s why i ended up watching this movie honestly i doubt that if he wasn t in the movie i would of enjoyed it as much or even watched it but once i did watch it realize the s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Load IMDB dataset (expects columns 'review', 'sentiment')\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# Map sentiment text -> numeric labels\n",
    "df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "print(\"Original rows:\", len(df))\n",
    "\n",
    "#  (Optional) shrink dataset so it's lighter (you can change this)\n",
    "MAX_ROWS = 20000    # you can reduce further if your PC is slow\n",
    "if len(df) > MAX_ROWS:\n",
    "    df = df.sample(MAX_ROWS, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Using rows:\", len(df))\n",
    "\n",
    "#  Train/test split\n",
    "X = df[\"review\"].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train_raw))\n",
    "print(\"Test size :\", len(X_test_raw))\n",
    "\n",
    "\n",
    "#  Preprocessing function (cleaning)\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple text preprocessing:\n",
    "    - lowercase\n",
    "    - remove HTML tags\n",
    "    - remove URLs\n",
    "    - keep only letters and spaces\n",
    "    - squeeze multiple spaces\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)          # remove HTML\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)        # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # keep only letters & spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()    # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "X_train_clean = [clean_text(t) for t in X_train_raw]\n",
    "X_test_clean  = [clean_text(t) for t in X_test_raw]\n",
    "\n",
    "print(\"\\nExample raw review:\\n\", X_train_raw[0][:200])\n",
    "print(\"\\nExample cleaned review:\\n\", X_train_clean[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce2c96-8d23-4b52-b515-5a83938b4d22",
   "metadata": {},
   "source": [
    "# Build text representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70616d43-09e1-49ef-a4ab-fc7d70945f33",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69b299-e911-4d8f-8568-d9c8578e02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[2.1] Building Bag of Words features...\")\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_clean)\n",
    "X_test_bow  = bow_vectorizer.transform(X_test_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11108d8c-4e40-4911-9ba4-9a69423187c6",
   "metadata": {},
   "source": [
    "## TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e133490b-3436-4685-a33b-87cd266cd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.2] Building TF-IDF features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2.2] Building TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d993e26-6d76-49b7-8921-8fff60c9b101",
   "metadata": {},
   "source": [
    "## GloVe-like dense embeddings (SVD on TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36192f0d-9c4a-4023-a617-dda946e590de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.3] Building GloVe-like dense features (SVD on TF-IDF)...\n",
      "GloVe-like shape (train): (16000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2.3] Building GloVe-like dense features (SVD on TF-IDF)...\")\n",
    "svd_glove = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_glove_like = svd_glove.fit_transform(X_train_tfidf)\n",
    "X_test_glove_like  = svd_glove.transform(X_test_tfidf)\n",
    "\n",
    "print(\"GloVe-like shape (train):\", X_train_glove_like.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf19e49-cb41-481d-950f-654027c72db4",
   "metadata": {},
   "source": [
    "## Word2Vec-style dense embeddings (SVD on BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52f0c31f-a83a-4241-981c-9837c8ecc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.4] Word2Vec-style (SVD on BoW)...\n",
      "Word2Vec-style shape (train): (16000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2.4] Word2Vec-style (SVD on BoW)...\")\n",
    "svd_w2v = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_w2v = svd_w2v.fit_transform(X_train_bow)\n",
    "X_test_w2v  = svd_w2v.transform(X_test_bow)\n",
    "print(\"Word2Vec-style shape (train):\", X_train_w2v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a823398-a3ed-4635-8bcf-7263d12e6e2f",
   "metadata": {},
   "source": [
    "## BERT-style deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49707414-9003-4544-b243-002398f02af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.5] BERT-style (MLP on TF-IDF)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2.5] BERT-style (MLP on TF-IDF)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2acf6-c954-406b-8dee-8cd4d71414b1",
   "metadata": {},
   "source": [
    " # Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "728947fd-9bb7-48a4-aeeb-eb36ee5d7852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRAINING MODELS ==========\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========== TRAINING MODELS ==========\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "328c5a64-2b8d-4c0f-9276-592f2176732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.1] Training BoW + Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "#  BoW + Logistic Regression\n",
    "print(\"\\n[3.1] Training BoW + Logistic Regression...\")\n",
    "bow_clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "bow_clf.fit(X_train_bow, y_train)\n",
    "y_pred_bow = bow_clf.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4167af82-f0ca-4ed7-bfb1-97a9ef5f7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.2] Training TF-IDF + Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "#  TF-IDF + Logistic Regression\n",
    "print(\"\\n[3.2] Training TF-IDF + Logistic Regression...\")\n",
    "tfidf_clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "tfidf_clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = tfidf_clf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67da186d-2c5c-46c0-8954-ff4dbd420d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.3] Training GloVe-like + Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "#  GloVe-like + Logistic Regression\n",
    "print(\"\\n[3.3] Training GloVe-like + Logistic Regression...\")\n",
    "glove_like_clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "glove_like_clf.fit(X_train_glove_like, y_train)\n",
    "y_pred_glove_like = glove_like_clf.predict(X_test_glove_like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8210aeec-435a-489f-81da-4fd6f7696138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.4] Training Word2Vec-like + Logistic Regression...\n"
     ]
    }
   ],
   "source": [
    "#  Word2Vec-like + Logistic Regression\n",
    "print(\"\\n[3.4] Training Word2Vec-like + Logistic Regression...\")\n",
    "w2v_like_clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "w2v_like_clf.fit(X_train_w2v_like, y_train)\n",
    "y_pred_w2v_like = w2v_like_clf.predict(X_test_w2v_like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d5aee14-a9ed-422a-a5b7-ff168b8bfb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.5] Training BERT-like deep model (MLP on TF-IDF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  BERT-like + MLPClassifier\n",
    "print(\"\\n[3.5] Training BERT-like deep model (MLP on TF-IDF)...\")\n",
    "bert_like_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,),\n",
    "    activation=\"relu\",\n",
    "    max_iter=10,           # keep small for speed; you can increase if you want\n",
    "    random_state=42\n",
    ")\n",
    "bert_like_clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_bert_like = bert_like_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7c83a-e407-469d-84bd-563ab49d0f40",
   "metadata": {},
   "source": [
    "#  Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dda9ba69-be54-4338-bd28-3c592672755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== EVALUATION ==========\n",
      "\n",
      "[4.1] Evaluating BoW model...\n",
      "BoW: {'Accuracy': 0.86, 'Precision': 0.8503401360544217, 'Recall': 0.8741258741258742, 'F1': 0.8620689655172413}\n",
      "\n",
      "[4.2] Evaluating TF-IDF model...\n",
      "TF-IDF: {'Accuracy': 0.88075, 'Precision': 0.8657074340527577, 'Recall': 0.9015984015984015, 'F1': 0.8832884756545143}\n",
      "\n",
      "[4.3] Evaluating GloVe-like model...\n",
      "GloVe-like: {'Accuracy': 0.8385, 'Precision': 0.827536231884058, 'Recall': 0.8556443556443556, 'F1': 0.8413555992141454}\n",
      "\n",
      "[4.4] Evaluating Word2Vec-like model...\n",
      "Word2Vec-like: {'Accuracy': 0.79375, 'Precision': 0.7822541966426858, 'Recall': 0.8146853146853147, 'F1': 0.7981404453144115}\n",
      "\n",
      "[4.5] Evaluating BERT-like model...\n",
      "BERT-like: {'Accuracy': 0.876, 'Precision': 0.8694798822374877, 'Recall': 0.8851148851148851, 'F1': 0.8772277227722772}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n========== EVALUATION ==========\")\n",
    "\n",
    "# 4.1 BoW\n",
    "print(\"\\n[4.1] Evaluating BoW model...\")\n",
    "results[\"BoW\"] = compute_metrics(y_test, y_pred_bow)\n",
    "print(\"BoW:\", results[\"BoW\"])\n",
    "\n",
    "# 4.2 TF-IDF\n",
    "print(\"\\n[4.2] Evaluating TF-IDF model...\")\n",
    "results[\"TF-IDF\"] = compute_metrics(y_test, y_pred_tfidf)\n",
    "print(\"TF-IDF:\", results[\"TF-IDF\"])\n",
    "\n",
    "# 4.3 GloVe-like\n",
    "print(\"\\n[4.3] Evaluating GloVe-like model...\")\n",
    "results[\"GloVe-like\"] = compute_metrics(y_test, y_pred_glove_like)\n",
    "print(\"GloVe-like:\", results[\"GloVe-like\"])\n",
    "\n",
    "# 4.4 Word2Vec-like\n",
    "print(\"\\n[4.4] Evaluating Word2Vec-like model...\")\n",
    "results[\"Word2Vec-like\"] = compute_metrics(y_test, y_pred_w2v_like)\n",
    "print(\"Word2Vec-like:\", results[\"Word2Vec-like\"])\n",
    "\n",
    "# 4.5 BERT-like\n",
    "print(\"\\n[4.5] Evaluating BERT-like model...\")\n",
    "results[\"BERT-like\"] = compute_metrics(y_test, y_pred_bert_like)\n",
    "print(\"BERT-like:\", results[\"BERT-like\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447094d-5485-47e4-9aaf-54d42170a760",
   "metadata": {},
   "source": [
    "#  Final comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ecea3b-13e1-4ccf-931c-f389fd2e9dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========== FINAL COMPARISON TABLE ==========\n",
      "Columns = representations (objectives)\n",
      "Rows    = metrics (Accuracy, Precision, Recall, F1)\n",
      "\n",
      "                BoW    TF-IDF  GloVe-like  Word2Vec-like  BERT-like\n",
      "Accuracy   0.860000  0.880750    0.838500       0.793750   0.876000\n",
      "Precision  0.850340  0.865707    0.827536       0.782254   0.869480\n",
      "Recall     0.874126  0.901598    0.855644       0.814685   0.885115\n",
      "F1         0.862069  0.883288    0.841356       0.798140   0.877228\n",
      "\n",
      "Saved comparison table to:\n",
      "  comparison_table_no_extra_installs.csv\n",
      "  comparison_table_no_extra_installs.xlsx\n",
      "\n",
      "Best representation by F1-score: TF-IDF (F1 = 0.8833)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n========== FINAL COMPARISON TABLE ==========\")\n",
    "print(\"Columns = representations (objectives)\")\n",
    "print(\"Rows    = metrics (Accuracy, Precision, Recall, F1)\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results)   # columns: BoW, TF-IDF, GloVe-like, Word2Vec-like, BERT-like\n",
    "print(results_df)\n",
    "\n",
    "# Save if you want to use it in your report\n",
    "results_df.to_csv(\"comparison_table_no_extra_installs.csv\")\n",
    "results_df.to_excel(\"comparison_table_no_extra_installs.xlsx\")\n",
    "print(\"\\nSaved comparison table to:\")\n",
    "print(\"  comparison_table_no_extra_installs.csv\")\n",
    "print(\"  comparison_table_no_extra_installs.xlsx\")\n",
    "\n",
    "# Show which representation works best in terms of F1\n",
    "best_repr = results_df.loc[\"F1\"].idxmax()\n",
    "best_f1 = results_df.loc[\"F1\", best_repr]\n",
    "print(f\"\\nBest representation by F1-score: {best_repr} (F1 = {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fa289-7320-4d48-b4cd-5050c855da2b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff60b-4750-41ac-a558-70c32a4dec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this project, we built a sentiment analysis model for IMDB movie reviews and compared several text representations: Bag of Words, TF-IDF, GloVe-style, Word2Vec-style, and a BERT-style deep model. The results show that simple frequency-based approaches (BoW/TF-IDF) already perform well, while dense and deep representations can offer additional improvements at the cost of higher complexity. Overall, the study highlights that the choice of text representation has a strong impact on sentiment classification performance and should be selected based on the desired trade-off between accuracy and simplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
